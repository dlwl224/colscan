# bot/bot_main5.py
# -*- coding: utf-8 -*-
import traceback
import os
import sys
import re
import warnings
import logging
import joblib
import json
import numpy as np
from typing import List, Dict, Any, Optional
from sklearn.metrics.pairwise import cosine_similarity

# GPU ì°¨ë‹¨ ë° ê²½ê³  ë¬´ì‹œ
os.environ.setdefault("CUDA_VISIBLE_DEVICES", "")
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
warnings.filterwarnings("ignore", category=FutureWarning, module="huggingface_hub.file_download")

from langchain.agents import Tool
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import PromptTemplate
from langchain.chains import ConversationalRetrievalChain
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.messages import HumanMessage, AIMessage
# .env ë¡œë“œ
load_dotenv('api.env')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# logging ì„¤ì •
logging.basicConfig(level=os.environ.get("LOG_LEVEL", "INFO"))
log = logging.getLogger("bot_main5")

# Redis memory
try:
    from bot.memory_redis import append_message, get_history, new_session_id, clear_session, touch_session
    REDIS_AVAILABLE = True
except Exception:
    REDIS_AVAILABLE = False

# 1) LLM ì´ˆê¸°í™”
if "GOOGLE_API_KEY" not in os.environ:
    raise ValueError("í™˜ê²½ ë³€ìˆ˜ì— 'GOOGLE_API_KEY'ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.01)

# 2) URL-BERT
from bot.tools.urlbert_tool import load_urlbert_tool
from urlbert.urlbert2.core.model_loader import GLOBAL_MODEL, GLOBAL_TOKENIZER
from bot.feature_extractor import build_raw_features, summarize_features_for_explanation

try:
    url_tool = load_urlbert_tool(GLOBAL_MODEL, GLOBAL_TOKENIZER)
    log.info("âœ… URL-BERT íˆ´ ë¡œë“œ ì™„ë£Œ")
except Exception as e:
    log.error(f"âŒ URL-BERT íˆ´ ë¡œë“œ ì‹¤íŒ¨: {e}")
    url_tool = Tool(
        name="URLBERT_ThreatAnalyzer",
        func=lambda x, _e=str(e): f"URL ë¶„ì„ íˆ´ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {_e}",
        description="URL ì•ˆì „/ìœ„í—˜ íŒë‹¨"
    )

# 3) RAG ì¸ë±ìŠ¤ ìƒì„±
RAG_INDEX_DIR = os.path.join(project_root, 'security_faiss_index')
RAG_DATA_PATH = os.path.join(project_root, 'data', 'rag_dataset.jsonl')
if not os.path.exists(RAG_INDEX_DIR) and os.path.exists(RAG_DATA_PATH):
    log.info(f"ğŸ”§ RAG ì¸ë±ìŠ¤ ìƒì„±: {RAG_DATA_PATH} -> {RAG_INDEX_DIR}")
    from bot.tools.rag_tools import build_rag_index_from_jsonl
    build_rag_index_from_jsonl(RAG_DATA_PATH, RAG_INDEX_DIR)

# 4) RAG ì²´ì¸ êµ¬ì„±
embeddings, vector_db, retriever, conversational_rag_chain = None, None, None, None
try:
    embeddings = HuggingFaceEmbeddings(model_name='jhgan/ko-sroberta-multitask')
    if os.path.exists(RAG_INDEX_DIR):
        vector_db = FAISS.load_local(RAG_INDEX_DIR, embeddings, allow_dangerous_deserialization=True)
        retriever = vector_db.as_retriever(search_type="mmr", search_kwargs={'k':5,'fetch_k':10})
        conversational_rag_chain = ConversationalRetrievalChain.from_llm(
            llm=llm, retriever=retriever, memory=None, return_source_documents=True, output_key='answer'
        )
        log.info("âœ… RAG ì²´ì¸ ë¡œë“œ ì™„ë£Œ")
except Exception as e:
    log.error(f"âŒ RAG ì²´ì¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    conversational_rag_chain = None

# 5) RAG/CHAT ë¼ìš°í„°
RAG_ROUTER_PATH = os.path.join(project_root, 'models', 'rag_chat_router.pkl')
try:
    RAG_CHAT_ROUTER_MODEL = joblib.load(RAG_ROUTER_PATH) if os.path.exists(RAG_ROUTER_PATH) else None
    if RAG_CHAT_ROUTER_MODEL:
        log.info("âœ… RAG-CHAT ë¼ìš°í„° ë¡œë“œ ì™„ë£Œ")
    else:
        log.warning("âš ï¸ RAG-CHAT ë¼ìš°í„° ì—†ìŒ, ê¸°ë³¸ CHAT ì‚¬ìš©")
except Exception as e:
    log.error(f"âŒ RAG-CHAT ë¼ìš°í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    RAG_CHAT_ROUTER_MODEL = None

# 6) Security centroid
SECURITY_CENTROID_PATH = os.path.join(project_root, 'models', 'security_centroid.npy')
try:
    SECURITY_CENTROID = np.load(SECURITY_CENTROID_PATH) if os.path.exists(SECURITY_CENTROID_PATH) else None
    if SECURITY_CENTROID is not None and SECURITY_CENTROID.ndim == 2 and SECURITY_CENTROID.shape[0] == 1:
        SECURITY_CENTROID = SECURITY_CENTROID.reshape(-1)
    log.info("âœ… Security centroid ë¡œë“œ ì™„ë£Œ")
except Exception as e:
    log.error(f"âŒ Security centroid ë¡œë“œ ì‹¤íŒ¨: {e}")
    SECURITY_CENTROID = None

# 7) Prompts / Keywords
from bot.prompts import SIMPLE_URL_PROMPT, URL_PROMPT, WHY_KEYWORDS, MEMORY_KEYWORDS
simple_url_prompt = SIMPLE_URL_PROMPT
url_prompt = URL_PROMPT

# 8) ì •ê·œì‹
URL_PATTERN = re.compile(r'(https?://\S+|(?:[A-Za-z0-9-]+\.)+[A-Za-z]{2,}\S*)')

# 9) verdict ì¶”ì¶œ
def _infer_verdict_from_text(bert_text: str) -> str:
    t = (bert_text or "").lower()
    bad_tokens = ["malicious", "phishing", "suspicious", "ì•…ì„±", "ìœ„í—˜", "ìœ í•´"]
    good_tokens = ["benign", "legitimate", "safe", "ì •ìƒ", "ì•ˆì „"]
    if any(tok in t for tok in bad_tokens) and not any(tok in t for tok in good_tokens):
        return "ì•…ì„±"
    if any(tok in t for tok in good_tokens) and not any(tok in t for tok in bad_tokens):
        return "ì •ìƒ"
    return "ì •ìƒ"

# 10) security embedding íŒë‹¨
def is_security_related_by_embedding(query: str, embedding_model, centroid, threshold=0.6) -> bool:
    q = (query or "").strip()
    if not q: return False
    try:
        if centroid is not None and embedding_model is not None:
            emb = embedding_model.embed_query(q)
            sim = cosine_similarity(np.array(emb).reshape(1,-1), np.array(centroid).reshape(1,-1))[0][0]
            return sim >= threshold
    except Exception:
        pass
    fallback = ["ë³´ì•ˆ","í•´í‚¹","ì·¨ì•½ì ","í”¼ì‹±","ëœì„¬ì›¨ì–´","ì•…ì„±","CVE"]
    return any(k in q for k in fallback)

# 11) history â†’ text
def history_to_text(chat_history: Optional[Any]) -> str:
    if not chat_history: return ""
    out = []
    if isinstance(chat_history, list):
        for m in chat_history:
            role = m.get("role","user") if isinstance(m, dict) else getattr(m,"role","user")
            content = m.get("text","") if isinstance(m, dict) else getattr(m,"content",None) or getattr(m,"text","")
            out.append(f"{role.upper()}: {content}")
    return "\n".join(out)


def format_history_for_langchain(chat_history: Optional[Any]) -> list:
    """
    [{'role': 'user', 'text': '...'}, ...] í˜•ì‹ì˜ ê¸°ë¡ì„
    [HumanMessage(...), AIMessage(...)] í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    if not chat_history:
        return []
    
    formatted_history = []
    for message in chat_history:
        role = message.get("role", "user")
        text = message.get("text", "")
        if role == "user":
            formatted_history.append(HumanMessage(content=text))
        else: # 'assistant', 'bot', 'ai' ë“± userê°€ ì•„ë‹Œ ëª¨ë“  ê²½ìš°
            formatted_history.append(AIMessage(content=text))
            
    return formatted_history
# 12) í•µì‹¬ í•¨ìˆ˜
# bot_main5.pyì˜ get_chatbot_response í•¨ìˆ˜ë¥¼ ì•„ë˜ ì½”ë“œë¡œ ì „ì²´ êµì²´í•´ì£¼ì„¸ìš”.

def get_chatbot_response(query: str, session_id: Optional[str] = None) -> Dict[str, Any]:
    """
    ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ì²˜ë¦¬í•˜ê³ , Redisë¥¼ ì´ìš©í•´ ëŒ€í™” ê¸°ë¡ì„ ê´€ë¦¬í•˜ì—¬ ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    text = (query or "").strip()
    if not text:
        return {"answer": "", "mode": "empty"}

    # --- 1. (LOAD) ëŒ€í™” ì‹œì‘ ì‹œ Redisì—ì„œ ì´ì „ ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ê¸° ---
    if session_id and REDIS_AVAILABLE:
        try:
            touch_session(session_id)
            chat_history = get_history(session_id)
        except Exception as e:
            log.error(f"Redisì—ì„œ ê¸°ë¡ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            chat_history = []
    else:
        chat_history = []

    history_text = history_to_text(chat_history)
    match = URL_PATTERN.search(text)
    is_why_question = any(k in text for k in WHY_KEYWORDS)
    is_memory_question = any(k in text for k in MEMORY_KEYWORDS)

    response = {}

    # --- 2. (PROCESS) ê¸°ì¡´ ë¡œì§ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„± ---
    # 1) ë©”ëª¨ë¦¬ ì§ˆë¬¸ ì²˜ë¦¬
    if is_memory_question and chat_history:
        memory_prompt = f"ë‹¹ì‹ ì€ ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ëŠ” ì¹œì ˆí•œ ì±—ë´‡ì…ë‹ˆë‹¤.\n\n[ëŒ€í™” ê¸°ë¡]\n{history_text}\n\n[ì§ˆë¬¸]\n{text}\n\n[ìµœì¢… ë‹µë³€]:"
        try:
            ans = llm.invoke(memory_prompt).content
            response = {"answer": ans, "mode": "memory"}
        except Exception as e:
            log.error(f"ë©”ëª¨ë¦¬ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ LLM í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            response = {"answer": "ê¸°ì–µì„ ë– ì˜¬ë¦¬ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”.", "mode": "memory_error"}

    # 2) URL ë¶„ì„ ì²˜ë¦¬
    elif match:
        url = match.group(1)
        try:
            bert_result = url_tool.func(url)
        except Exception as e:
            bert_result = f"URL-BERT ì˜¤ë¥˜: {e}"

        if is_why_question:
            # ... ìƒì„¸ ë¶„ì„ ë¡œì§ ...
            try:
                df = build_raw_features(url)
                verdict = _infer_verdict_from_text(bert_result)
                reasons = summarize_features_for_explanation(df, verdict, top_k=3) if not df.empty else ["ì„¸ë¶€ íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨"]
                feature_details = "\n".join(f"- {r}" for r in reasons)
                prompt = url_prompt.format(user_query=text, bert_result=bert_result, feature_details=feature_details)
                ans = llm.invoke(prompt).content
                response = {"answer": ans, "mode": "url_analysis_detailed", "url": url}
            except Exception as e:
                response = {"answer": "URL ìƒì„¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”.", "mode": "url_error"}
        else:
            # ... ê°„ë‹¨ ë¶„ì„ ë¡œì§ ...
            prompt = simple_url_prompt.format(bert_result=bert_result)
            try:
                ans = llm.invoke(prompt).content
                response = {"answer": ans, "mode": "url_analysis_simple", "url": url}
            except Exception as e:
                response = {"answer": "URLì„ ë¶„ì„í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”.", "mode": "url_error"}

    # --- âœ¨ 3) RAG / CHAT / ê°€ë“œë ˆì¼ ì²˜ë¦¬ ---
    else:
        # ë¼ìš°í„°ê°€ RAGë¥¼ ì¶”ì²œí•˜ëŠ”ì§€ ë¨¼ì € í™•ì¸
        action = "CHAT"
        if RAG_CHAT_ROUTER_MODEL:
            try:
                action = RAG_CHAT_ROUTER_MODEL.predict([text])[0]
            except Exception:
                action = "CHAT"

        # ë¼ìš°í„°ì™€ ë³„ê°œë¡œ, ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ë³´ì•ˆ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ í™•ì¸
        sec_flag = False
        try:
            sec_flag = is_security_related_by_embedding(text, embeddings, SECURITY_CENTROID)
        except Exception:
            sec_flag = any(k in text for k in ["ë³´ì•ˆ", "í•´í‚¹", "ì·¨ì•½ì ", "í”¼ì‹±", "ëœì„¬ì›¨ì–´", "CVE"])

        # <ì¡°ê±´> ë¼ìš°í„°ê°€ 'RAG'ì´ê±°ë‚˜, ë‚´ìš© ìì²´ê°€ 'ë³´ì•ˆ ê´€ë ¨'ì¼ ê²½ìš° -> RAGë¡œ ë‹µë³€ ì‹œë„
        if (action == "RAG" or sec_flag) and conversational_rag_chain:
            try:
                langchain_chat_history = format_history_for_langchain(chat_history)
                res = conversational_rag_chain.invoke({
                    "question": text,
                    "chat_history": langchain_chat_history
                })
                sources = [doc.metadata.get("source") for doc in res.get("source_documents", []) if doc.metadata.get("source")]
                response = {"answer": res.get("answer", ""), "mode": "rag", "sources": list(set(sources))}
            except Exception as e:
                log.error(f"RAG ì²´ì¸ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                response = {"answer": "ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”.", "mode": "rag_error"}
        # <ì¡°ê±´> ìœ„ ê²½ìš°ê°€ ì•„ë‹ ê²½ìš° (ë³´ì•ˆê³¼ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸) -> ê°€ë“œë ˆì¼ ë©”ì‹œì§€ ì¶œë ¥
        else: 
            GREETING_KEYWORDS = ["ì•ˆë…•", "í•˜ì´", "ã…ã…‡", "hi", "hello"]
            if text.lower() in GREETING_KEYWORDS:
                friendly_greeting = (
                    "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? "
                    "íì‹±, í”¼ì‹±, URL ë³´ì•ˆ ë“± ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ë¬¼ì–´ë³´ì„¸ìš”."
                )
                response = {"answer": friendly_greeting, "mode": "chat_greeting"}
            else:
                response = {"answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì €ëŠ” ë³´ì•ˆ ì „ë¬¸ ì±—ë´‡ìœ¼ë¡œ, ë³´ì•ˆ ê´€ë ¨ ì§ˆë¬¸ì—ë§Œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ", "mode": "chat_guardrail"}


    # --- 4. (SAVE) ì´ë²ˆ ëŒ€í™”ë¥¼ Redisì— ì €ì¥í•˜ê¸° ---
    if session_id and REDIS_AVAILABLE and response.get("answer"):
        try:
            append_message(session_id, "user", query)
            append_message(session_id, "assistant", response["answer"])
        except Exception as e:
            log.error(f"Redisì— ëŒ€í™” ê¸°ë¡ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    return response

if __name__=="__main__":
    print("--- ì±—ë´‡ ì‹œì‘ (ì¢…ë£Œ: 'ì¢…ë£Œ') ---")
    while True:
        try: text=input("You â–¶ ").strip()
        except (EOFError,KeyboardInterrupt): break
        if text.lower() in ["ì¢…ë£Œ","exit"]: break
        if not text: continue
        res=get_chatbot_response(text)
        print(f"Bot â–¶ {res.get('answer')}\n")
