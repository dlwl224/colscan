# bot/bot_main5.py
# -*- coding: utf-8 -*-
import traceback
import os
import sys
import re
import warnings
import logging
import joblib
import json
import numpy as np
from typing import List, Dict, Any, Optional
from sklearn.metrics.pairwise import cosine_similarity

# GPU ì°¨ë‹¨ ë° ê²½ê³  ë¬´ì‹œ
os.environ.setdefault("CUDA_VISIBLE_DEVICES", "")
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
warnings.filterwarnings("ignore", category=FutureWarning, module="huggingface_hub.file_download")

from langchain.agents import Tool
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import PromptTemplate
from langchain.chains import ConversationalRetrievalChain
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.messages import HumanMessage, AIMessage
# .env ë¡œë“œ
load_dotenv('api.env')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# logging ì„¤ì •
logging.basicConfig(level=os.environ.get("LOG_LEVEL", "INFO"))
log = logging.getLogger("bot_main5")

# Redis memory
try:
    from bot.memory_redis import append_message, get_history, new_session_id, clear_session, touch_session
    REDIS_AVAILABLE = True
except Exception:
    REDIS_AVAILABLE = False

# 1) LLM ì´ˆê¸°í™”
if "GOOGLE_API_KEY" not in os.environ:
    raise ValueError("í™˜ê²½ ë³€ìˆ˜ì— 'GOOGLE_API_KEY'ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.01)

# 2) URL-BERT
from bot.tools.urlbert_tool import load_urlbert_tool
from urlbert.urlbert2.core.model_loader import GLOBAL_MODEL, GLOBAL_TOKENIZER
from bot.feature_extractor import build_raw_features, summarize_features_for_explanation

try:
    url_tool = load_urlbert_tool(GLOBAL_MODEL, GLOBAL_TOKENIZER)
    log.info("âœ… URL-BERT íˆ´ ë¡œë“œ ì™„ë£Œ")
except Exception as e:
    log.error(f"âŒ URL-BERT íˆ´ ë¡œë“œ ì‹¤íŒ¨: {e}")
    url_tool = Tool(
        name="URLBERT_ThreatAnalyzer",
        func=lambda x, _e=str(e): f"URL ë¶„ì„ íˆ´ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {_e}",
        description="URL ì•ˆì „/ìœ„í—˜ íŒë‹¨"
    )

# 3) RAG ì¸ë±ìŠ¤ ìƒì„±
RAG_INDEX_DIR = os.path.join(project_root, 'security_faiss_index')
RAG_DATA_PATH = os.path.join(project_root, 'data', 'rag_dataset.jsonl')
if not os.path.exists(RAG_INDEX_DIR) and os.path.exists(RAG_DATA_PATH):
    log.info(f"ğŸ”§ RAG ì¸ë±ìŠ¤ ìƒì„±: {RAG_DATA_PATH} -> {RAG_INDEX_DIR}")
    from bot.tools.rag_tools import build_rag_index_from_jsonl
    build_rag_index_from_jsonl(RAG_DATA_PATH, RAG_INDEX_DIR)

# 4) RAG ì²´ì¸ êµ¬ì„±
embeddings, vector_db, retriever, conversational_rag_chain = None, None, None, None
try:
    embeddings = HuggingFaceEmbeddings(model_name='jhgan/ko-sroberta-multitask')
    if os.path.exists(RAG_INDEX_DIR):
        vector_db = FAISS.load_local(RAG_INDEX_DIR, embeddings, allow_dangerous_deserialization=True)
        retriever = vector_db.as_retriever(search_type="mmr", search_kwargs={'k':5,'fetch_k':10})
        conversational_rag_chain = ConversationalRetrievalChain.from_llm(
            llm=llm, retriever=retriever, memory=None, return_source_documents=True, output_key='answer'
        )
        log.info("âœ… RAG ì²´ì¸ ë¡œë“œ ì™„ë£Œ")
except Exception as e:
    log.error(f"âŒ RAG ì²´ì¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    conversational_rag_chain = None

# 5) RAG/CHAT ë¼ìš°í„°
RAG_ROUTER_PATH = os.path.join(project_root, 'models', 'rag_chat_router.pkl')
try:
    RAG_CHAT_ROUTER_MODEL = joblib.load(RAG_ROUTER_PATH) if os.path.exists(RAG_ROUTER_PATH) else None
    if RAG_CHAT_ROUTER_MODEL:
        log.info("âœ… RAG-CHAT ë¼ìš°í„° ë¡œë“œ ì™„ë£Œ")
    else:
        log.warning("âš ï¸ RAG-CHAT ë¼ìš°í„° ì—†ìŒ, ê¸°ë³¸ CHAT ì‚¬ìš©")
except Exception as e:
    log.error(f"âŒ RAG-CHAT ë¼ìš°í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    RAG_CHAT_ROUTER_MODEL = None

# 6) Security centroid
SECURITY_CENTROID_PATH = os.path.join(project_root, 'models', 'security_centroid.npy')
try:
    SECURITY_CENTROID = np.load(SECURITY_CENTROID_PATH) if os.path.exists(SECURITY_CENTROID_PATH) else None
    if SECURITY_CENTROID is not None and SECURITY_CENTROID.ndim == 2 and SECURITY_CENTROID.shape[0] == 1:
        SECURITY_CENTROID = SECURITY_CENTROID.reshape(-1)
    log.info("âœ… Security centroid ë¡œë“œ ì™„ë£Œ")
except Exception as e:
    log.error(f"âŒ Security centroid ë¡œë“œ ì‹¤íŒ¨: {e}")
    SECURITY_CENTROID = None

# 7) Prompts / Keywords
from bot.prompts import SIMPLE_URL_PROMPT, URL_PROMPT, WHY_KEYWORDS, MEMORY_KEYWORDS
simple_url_prompt = SIMPLE_URL_PROMPT
url_prompt = URL_PROMPT

# 8) ì •ê·œì‹
URL_PATTERN = re.compile(r'(https?://\S+|(?:[A-Za-z0-9-]+\.)+[A-Za-z]{2,}\S*)')

# 9) verdict ì¶”ì¶œ
def _infer_verdict_from_text(bert_text: str) -> str:
    t = (bert_text or "").lower()
    bad_tokens = ["malicious", "phishing", "suspicious", "ì•…ì„±", "ìœ„í—˜", "ìœ í•´"]
    good_tokens = ["benign", "legitimate", "safe", "ì •ìƒ", "ì•ˆì „"]
    if any(tok in t for tok in bad_tokens) and not any(tok in t for tok in good_tokens):
        return "ì•…ì„±"
    if any(tok in t for tok in good_tokens) and not any(tok in t for tok in bad_tokens):
        return "ì •ìƒ"
    return "ì •ìƒ"

# 10) security embedding íŒë‹¨
def is_security_related_by_embedding(query: str, embedding_model, centroid, threshold=0.6) -> bool:
    q = (query or "").strip()
    if not q: return False
    try:
        if centroid is not None and embedding_model is not None:
            emb = embedding_model.embed_query(q)
            sim = cosine_similarity(np.array(emb).reshape(1,-1), np.array(centroid).reshape(1,-1))[0][0]
            return sim >= threshold
    except Exception:
        pass
    fallback = ["ë³´ì•ˆ","í•´í‚¹","ì·¨ì•½ì ","í”¼ì‹±","ëœì„¬ì›¨ì–´","ì•…ì„±","CVE"]
    return any(k in q for k in fallback)

# 11) history â†’ text
def history_to_text(chat_history: Optional[Any]) -> str:
    if not chat_history: return ""
    out = []
    if isinstance(chat_history, list):
        for m in chat_history:
            role = m.get("role","user") if isinstance(m, dict) else getattr(m,"role","user")
            content = m.get("text","") if isinstance(m, dict) else getattr(m,"content",None) or getattr(m,"text","")
            out.append(f"{role.upper()}: {content}")
    return "\n".join(out)


def format_history_for_langchain(chat_history: Optional[Any]) -> list:
    """
    [{'role': 'user', 'text': '...'}, ...] í˜•ì‹ì˜ ê¸°ë¡ì„
    [HumanMessage(...), AIMessage(...)] í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    if not chat_history:
        return []
    
    formatted_history = []
    for message in chat_history:
        role = message.get("role", "user")
        text = message.get("text", "")
        if role == "user":
            formatted_history.append(HumanMessage(content=text))
        else: # 'assistant', 'bot', 'ai' ë“± userê°€ ì•„ë‹Œ ëª¨ë“  ê²½ìš°
            formatted_history.append(AIMessage(content=text))
            
    return formatted_history
# 12) í•µì‹¬ í•¨ìˆ˜
def get_chatbot_response(query: str, chat_history: Optional[Any]=None, session_id: Optional[str]=None) -> Dict[str, Any]:
    text = (query or "").strip()
    if not text: return {"answer": "", "mode": "empty"}

    # Redis ì„¸ì…˜
    if session_id and REDIS_AVAILABLE:
        try: touch_session(session_id)
        except Exception: pass

    history_text = history_to_text(chat_history)
    match = URL_PATTERN.search(text)
    is_why_question = any(k in text for k in WHY_KEYWORDS)
    is_memory_question = any(k in text for k in MEMORY_KEYWORDS)

    # 1) ë©”ëª¨ë¦¬ ì§ˆë¬¸
    if is_memory_question and chat_history:
        memory_prompt = f"ë‹¹ì‹ ì€ ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ëŠ” ì¹œì ˆí•œ ì±—ë´‡ì…ë‹ˆë‹¤.\n\n[ëŒ€í™” ê¸°ë¡]\n{history_text}\n\n[ì§ˆë¬¸]\n{text}\n\n[ìµœì¢… ë‹µë³€]:"
        try: ans = llm.invoke(memory_prompt).content
        except Exception: ans = "ë©”ëª¨ë¦¬ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"
        return {"answer": ans, "mode":"memory"}

    # 2) URL ë¶„ì„
    if match:
        url = match.group(1)
        try: bert_result = url_tool.func(url)
        except Exception as e: bert_result = f"URL-BERT ì˜¤ë¥˜: {e}"

        if is_why_question:
            try:
                df = build_raw_features(url)
                verdict = _infer_verdict_from_text(bert_result)
                reasons = summarize_features_for_explanation(df, verdict, top_k=3) if not df.empty else ["ì„¸ë¶€ íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨"]
                feature_details = "\n".join(f"- {r}" for r in reasons)
            except Exception as e:
                feature_details = f"ì„¸ë¶€ íŠ¹ì§• ì¶”ì¶œ ì˜¤ë¥˜: {e}"

            prompt = url_prompt.format(user_query=text, bert_result=bert_result, feature_details=feature_details)
            try: ans = llm.invoke(prompt).content
            except Exception: ans = "ìƒì„¸ ë¶„ì„ ìƒì„± ì‹¤íŒ¨"
            return {"answer": ans, "mode":"url_analysis_detailed","url":url}
        else:
            prompt = simple_url_prompt.format(bert_result=bert_result)
            try: ans = llm.invoke(prompt).content
            except Exception: ans = "ê°„ë‹¨ ë¶„ì„ ìƒì„± ì‹¤íŒ¨"
            return {"answer": ans, "mode":"url_analysis_simple","url":url}

    # 3) RAG vs CHAT ë¼ìš°í„°
    action = "CHAT"
    if RAG_CHAT_ROUTER_MODEL:
        try:
            action = RAG_CHAT_ROUTER_MODEL.predict([text])[0]
            log.info(f"RAG_ROUTER predict -> {action} for text: {text!r}")
        except Exception as e:
            log.exception(f"RAG router predict ì‹¤íŒ¨: {e}")
            action = "CHAT"

    # === ìƒˆë¡œ ì¶”ê°€í•œ ë¶€ë¶„: retriever / FAISS ìƒíƒœ ì ê²€ ë¡œê¹… ===
    try:
        # FAISS ì¸ë±ìŠ¤ ì´ ë¬¸ì„œ ìˆ˜ ì¶œë ¥ (ê°€ëŠ¥í•˜ë©´)
        if vector_db is not None:
            try:
                idx = getattr(vector_db, "index", None)
                if idx is not None and hasattr(idx, "ntotal"):
                    log.info(f"FAISS index ntotal: {idx.ntotal}")
                else:
                    log.info("vector_db.index ë˜ëŠ” ntotal ì†ì„± ì—†ìŒ")
            except Exception:
                log.exception("FAISS ntotal ì¡°íšŒ ì¤‘ ì˜¤ë¥˜")

        # retriever í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
        if retriever is not None:
            try:
                sample_docs = retriever.get_relevant_documents(text)
                log.info(f"retriever.get_relevant_documents -> {len(sample_docs)} docs")
                for i, d in enumerate(sample_docs[:3]):
                    meta = getattr(d, "metadata", {})
                    snippet = (getattr(d, "page_content", "") or "")[:200]
                    log.info(f"doc[{i}] source={meta.get('source')} snippet={snippet!r}")
            except Exception:
                log.exception("retriever.get_relevant_documents í˜¸ì¶œ ì‹¤íŒ¨")
    except Exception:
        log.exception("retriever/FAISS ìƒíƒœ ì ê²€ ì¤‘ ì˜ˆì™¸")

    # RAG í˜¸ì¶œ ë¸”ë¡ (ì˜ˆì™¸ ì‹œ ìŠ¤íƒíŠ¸ë ˆì´ìŠ¤ ë¡œê¹… + ê°œë°œì‹œ ìƒì„¸ ë°˜í™˜)
    if action == "RAG" and conversational_rag_chain:
        try:
            log.info(f"RAG í˜¸ì¶œ ì‹œë„: text={text!r}")
            # LangChainì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ëŒ€í™” ê¸°ë¡ ë³€í™˜
            langchain_chat_history = format_history_for_langchain(chat_history)
            
            # invoke ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì²´ì¸ í˜¸ì¶œ
            res = conversational_rag_chain.invoke({
                "question": text,
                "chat_history": langchain_chat_history
            })
            log.info("RAG ì²´ì¸ í˜¸ì¶œ ì„±ê³µ")
            sources = []
            for doc in res.get("source_documents", []):
                try: src = getattr(doc, "metadata", {}).get("source")
                except Exception: src = None
                if src and src not in sources: sources.append(src)
            return {"answer": res.get("answer",""), "mode":"rag", "sources":sources}

        except Exception as e:
            tb = traceback.format_exc()
            log.error(f"RAG ì²´ì¸ í˜¸ì¶œ ì‹¤íŒ¨: {e}\n{tb}")
            # ê°œë°œ í™˜ê²½ì´ë©´ ìƒì„¸ ì—ëŸ¬ ë°˜í™˜
            if os.environ.get("DEBUG", "").lower() in ("1", "true", "yes"):
                return {"answer": f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ (ì²´ì¸ í˜¸ì¶œ ì‹¤íŒ¨): {e}\n{tb}", "mode":"rag_error"}
            return {"answer":"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ (ì²´ì¸ í˜¸ì¶œ ì‹¤íŒ¨)", "mode":"rag_error"}

    else:
        # RAG ë¶„ê¸°ë¡œ ì•ˆ ê°€ëŠ” ê²½ìš°(ì¼ë°˜ CHAT ë˜ëŠ” ë³´ì•ˆ í”Œë˜ê·¸)
        sec_flag = False
        try: sec_flag = is_security_related_by_embedding(text, embeddings, SECURITY_CENTROID)
        except Exception:
            sec_flag = any(k in text for k in ["ë³´ì•ˆ","í•´í‚¹","ì·¨ì•½ì ","í”¼ì‹±","ëœì„¬ì›¨ì–´","CVE"])

        if sec_flag and conversational_rag_chain:
            try:
                # ì•ˆì „í•˜ê²Œ invoke í˜¸ì¶œ ì‹œë„
                try:
                    res = conversational_rag_chain.invoke({"question":text,"chat_history":chat_history})
                except Exception:
                    # ë§ˆì§€ë§‰ ë³´ë£¨: __call__ ì‹œë„
                    res = conversational_rag_chain({"question":text,"chat_history":chat_history})
            except Exception as e:
                log.exception(f"sec_flag ìƒíƒœì—ì„œ conversational_rag_chain í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                res = {"answer":"ì±—ë´‡ ì˜¤ë¥˜"}
            return {"answer": res.get("answer",""), "mode":"chat"}
        else:
            try:
                ans = llm.invoke(f"[ëŒ€í™” ê¸°ë¡]\n{history_text}\n{text}").content
            except Exception:
                ans = "ë‚´ë¶€ ì˜¤ë¥˜"
            return {"answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì €ëŠ” ë³´ì•ˆ ì „ë¬¸ ì±—ë´‡ìœ¼ë¡œ, ë³´ì•ˆ ê´€ë ¨ ì§ˆë¬¸ì—ë§Œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.", "mode":"chat_guardrail"}


if __name__=="__main__":
    print("--- ì±—ë´‡ ì‹œì‘ (ì¢…ë£Œ: 'ì¢…ë£Œ') ---")
    while True:
        try: text=input("You â–¶ ").strip()
        except (EOFError,KeyboardInterrupt): break
        if text.lower() in ["ì¢…ë£Œ","exit"]: break
        if not text: continue
        res=get_chatbot_response(text)
        print(f"Bot â–¶ {res.get('answer')}\n")
